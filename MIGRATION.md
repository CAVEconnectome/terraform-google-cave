# CAVE on GCP – Migration from CAVEdeployment

This guide describes how to migrate an existing CAVE system to  a CAVE environment managed with Terragrunt and Helmfile.

In contrast to QUICKSTART.md this will assume you have a fully functioning local and global cluster, and want to spin up new clusters that utilize the new infrastructure. 

At a glance:
- Terragrunt + Terraform: provisions everything outside Kubernetes (SQL, Redis, networks, DNS, Pub/Sub, buckets, service accounts, IAM).
- Helmfile + Helm: deploys CAVE apps into Kubernetes using values generated by the Terraform modules.
- Helm charts live in the companion repo: https://github.com/CAVEconnectome/cave-helm-charts

Here's an overview picture of the architecture and where various things live within it. 
![CAVE on GCP – Architecture](docs/architecture.png)

## Prerequisites
- macOS with Homebrew
- Install tools:
  - `brew install terraform terragrunt kubectl helm helmfile jq direnv sops gitleaks`
- Authenticate:
  - `gcloud auth login`
  - `gcloud auth application-default login`
  - `gcloud config set project <PROJECT_ID>`

we assume you have google cloud sdk installed but if you didn't
- `brew install --cask google-cloud-sdk`

## Enable APIs
These services are likely already enabled on your project, but secretmanager was not used in CAVEdeployment style deployments.
Re-enabling services won't break anything.
```
gcloud services enable \
  compute.googleapis.com container.googleapis.com dns.googleapis.com \
  redis.googleapis.com sqladmin.googleapis.com secretmanager.googleapis.com \
  iam.googleapis.com cloudresourcemanager.googleapis.com
```

## Create Terraform state bucket
This is where the state of the current deployment(s) will be stored, everyone who is going to be using terragrunt/terraform will need to have read and write access to this bucket. 

To create the bucket you can use this line, but you'll have to add permissions on the bucket separately using the command line or cloud console. 
```
gsutil mb -l us gs://<STATE_BUCKET>
```

## Create DB credentials secret
Rather than store the cloudsql username and password that you setup in an environment file, these scripts assume that the credentials are stored in a google secret manager credential.  So you should take your existing password from your environment file and replace it here.  ENV refers to the "local-environment-name" or "global-environment-name" you have used before and will fill into the cookiecutter.  This would be the part of the domain that you had persist from cluster to cluster (as opposed to the cluster specific name).

You will have to do this twice for the username/password on the global and local sql server.
```
printf '%s' '{"username":"postgres","password":"<YOUR-EXISTING-POSTGRES-PASSWORD>"}' \
 | gcloud secrets create <ENV>-postgres-credentials --data-file=-
```
## Environment repository
We reccomend that you setup a seperate repository to store all your environment configurations. So make a new folder that you will store and track these files. This is the equivalent to the ENV_REPO that was specified in your CAVEdeployment environment.  However, unlike the environment files, we have tried to design this repo so that no sensitive data is stored in this repository, so it can be developed in the open. 


## Generate a starter local environment with Cookiecutter
We would reccomend migrating a local cluster first, before a global one as you'll get the most value out of managing regular updates to services.  You can continue to manage your global cluster using CAVEdeployment, as they have no real interactions. 

copy the example cookiecutter template file from this repository (example-local-config.yaml) into that directory (we will refer to this location as ENVIRONMENTS_REPO) and rename it to something (we will call it ENV_CONFIG.yaml)

install cookiecutter however you would like(for example)
```
pipx install cookiecutter  # or pip install --user cookiecutter
```

cd into ENVIRONMENTS_REPO

```
cookiecutter terraform-google-cave/cookiecutter_templates/terragrunt-env
```

Answer prompts: repo_name, org, environment, project_id, region, zone, etc.

## Import existing resources (optional)
You will want to do this if you are migrating from existing infrastructure that has data you don't want to lose.  Make sure the names of everything are aligned with what actually exists, which might requires careful editing of the root.hcl and terragrunt.hcl contained variables. 
```
cd <repo_name>/environments/<org>/static
../scripts/terragrunt_import_sql.sh
terragrunt plan -refresh-only
```

## Provision
```
cd <repo_name>/environments/<org>/static
terragrunt init && terragrunt apply

cd ../<cluster_prefix>
terragrunt init && terragrunt apply
```

## Deploy apps with Helmfile
- Install the Helm Diff plugin (Helmfile uses `helm diff` for planning):
  - `/opt/homebrew/bin/helm plugin install https://github.com/databus23/helm-diff`
  - Verify: `/opt/homebrew/bin/helm plugin list` (should list `diff`)
  - If already installed: `/opt/homebrew/bin/helm plugin update diff`

```
cd <repo_name>/environments/<org>/<cluster_prefix>/helmfile
cp helmfile.yaml.example helmfile.yaml
./configure.sh
# Edit helmfile.yaml and create overrides (e.g., materialize.yaml) as needed
helmfile apply
# Tip: If you cannot install the plugin, use --skip-diff as a temporary workaround
# helmfile apply --skip-diff
```



## How the pieces fit (modules and environments)

For each cluster, there are two Terraform module layers coordinated by Terragrunt:
- infrastructure: long-lived shared services (Cloud SQL, Redis, VPC, DNS, buckets, Pub/Sub, etc.). Persist across cluster re-creations.
- cluster: the GKE cluster and templated Helm values derived from infra outputs. This layer prepares the in-cluster prerequisites and writes defaults consumed by Helmfile.

In CAVE, there are both local and global clusters, so there will be four modules overall: local_infrastructure, local_cluster, global_infrastructure, global_cluster. Terragrunt shares common inputs (project_id, region, cluster_prefix) across them and manages ordering.

The Terraform modules render values files and templates that Helmfile consumes (for example, queue/exchange names, Bigtable instance, Redis host, domainName, Workload Identity SAs). This wiring lets the Helm charts connect to the external infrastructure.

Helm charts are developed for Google Cloud (GKE) and may need adjustments on other Kubernetes platforms (e.g., scalers, IAM annotations, ingress classes). See the cave-helm-charts README for details.

## Security checklist for making repos public
- Scan for secrets: `gitleaks detect -v`
- Ensure no tfstate files are committed
- Use Secret Manager or SOPS for any app secrets
- Consider exposing project IDs acceptable

## Troubleshooting
- Clear caches if provider/schema errors:
  - `rm -rf .terragrunt-cache` and re-run `terragrunt init -upgrade`
- Verify kube credentials: `kubectl get ns`
- Check Helm repos: `helm repo add jetstack https://charts.jetstack.io && helm repo update`

## New to Terraform/Terragrunt/Helm?
- Terraform: https://developer.hashicorp.com/terraform/docs
- Terragrunt (wrapper for DRY Terraform): https://terragrunt.gruntwork.io/docs/
- Helm (Kubernetes package manager): https://helm.sh/docs/
- Helmfile (stateful Helm deployments): https://helmfile.readthedocs.io/

This repo’s Cookiecutter template scaffolds a starter environment with sensible defaults and scripts to import existing resources into Terraform state if you are migrating from legacy setups.
